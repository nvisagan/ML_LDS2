{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_kaggle_challenge_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvisagan/DS-Unit-2-Kaggle-Challenge/blob/master/module1/assignment_kaggle_challenge_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXUfiQ2UKj6",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science, Unit 2: Predictive Modeling\n",
        "\n",
        "# Kaggle Challenge, Module 1\n",
        "\n",
        "## Assignment\n",
        "- [ ] Do train/validate/test split with the Tanzania Waterpumps data.\n",
        "- [ ] Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features. (For example, [what other columns have zeros and shouldn't?](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values) What other columns are duplicates, or nearly duplicates? Can you extract the year from date_recorded? Can you engineer new features, such as the number of years from waterpump construction to waterpump inspection?)\n",
        "- [ ] Select features. Use a scikit-learn pipeline to encode categoricals, impute missing values, and fit a decision tree classifier.\n",
        "- [ ] Get your validation accuracy score.\n",
        "- [ ] Get and plot your feature importances.\n",
        "- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
        "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "\n",
        "## Stretch Goals\n",
        "\n",
        "### Reading\n",
        "\n",
        "- A Visual Introduction to Machine Learning\n",
        "  - [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "  - [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
        "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) — _Don’t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
        "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)\n",
        "\n",
        "\n",
        "### Doing\n",
        "- [ ] Add your own stretch goal(s) !\n",
        "- [ ] Try other [scikit-learn imputers](https://scikit-learn.org/stable/modules/impute.html).\n",
        "- [ ] Try other [scikit-learn scalers](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "- [ ] Make exploratory visualizations and share on Slack.\n",
        "\n",
        "\n",
        "#### Exploratory visualizations\n",
        "\n",
        "Visualize the relationships between feature(s) and target. I recommend you do this with your training set, after splitting your data. \n",
        "\n",
        "For this problem, you may want to create a new column to represent the target as a number, 0 or 1. For example:\n",
        "\n",
        "```python\n",
        "train['functional'] = (train['status_group']=='functional').astype(int)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "You can try [Seaborn \"Categorical estimate\" plots](https://seaborn.pydata.org/tutorial/categorical.html) for features with reasonably few unique values. (With too many unique values, the plot is unreadable.)\n",
        "\n",
        "- Categorical features. (If there are too many unique values, you can replace less frequent values with \"OTHER.\")\n",
        "- Numeric features. (If there are too many unique values, you can [bin with pandas cut / qcut functions](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html?highlight=qcut#discretization-and-quantiling).)\n",
        "\n",
        "You can try [Seaborn linear model plots](https://seaborn.pydata.org/tutorial/regression.html) with numeric features. For this problem, you may want to use the parameter `logistic=True`\n",
        "\n",
        "You do _not_ need to use Seaborn, but it's nice because it includes confidence intervals to visualize uncertainty.\n",
        "\n",
        "#### High-cardinality categoricals\n",
        "\n",
        "This code from a previous assignment demonstrates how to replace less frequent values with 'OTHER'\n",
        "\n",
        "```python\n",
        "# Reduce cardinality for NEIGHBORHOOD feature ...\n",
        "\n",
        "# Get a list of the top 10 neighborhoods\n",
        "top10 = train['NEIGHBORHOOD'].value_counts()[:10].index\n",
        "\n",
        "# At locations where the neighborhood is NOT in the top 10,\n",
        "# replace the neighborhood with 'OTHER'\n",
        "train.loc[~train['NEIGHBORHOOD'].isin(top10), 'NEIGHBORHOOD'] = 'OTHER'\n",
        "test.loc[~test['NEIGHBORHOOD'].isin(top10), 'NEIGHBORHOOD'] = 'OTHER'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9eSnDYhUGD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecc7641f-d83c-4ef1-cd1c-fa8385c9dd8f"
      },
      "source": [
        "# If you're in Colab...\n",
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "    # Install required python packages:\n",
        "    # category_encoders, version >= 2.0\n",
        "    # pandas-profiling, version >= 2.0\n",
        "    # plotly, version >= 4.0\n",
        "    !pip install --upgrade category_encoders pandas-profiling plotly\n",
        "    \n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module1')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/a1/f7a22f144f33be78afeb06bfa78478e8284a64263a3c09b1ef54e673841e/category_encoders-2.0.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.3MB/s \n",
            "\u001b[?25hCollecting pandas-profiling\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/2f/aae19e2173c10a9bb7fee5f5cad35dbe53a393960fc91abc477dcc4661e8/pandas-profiling-2.3.0.tar.gz (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.9MB/s \n",
            "\u001b[?25hCollecting plotly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/19/8437e22c84083a6d5d8a3c80f4edc73c9dcbb89261d07e6bd13b48752bbd/plotly-4.1.1-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=1.4 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.0.3)\n",
            "Requirement already satisfied, skipping upgrade: jinja2>=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (2.10.1)\n",
            "Requirement already satisfied, skipping upgrade: missingno>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (0.4.2)\n",
            "Collecting htmlmin>=0.1.12 (from pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
            "Collecting phik>=0.9.8 (from pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/ad/24a16fa4ba612fb96a3c4bb115a5b9741483f53b66d3d3afd987f20fa227/phik-0.9.8-py3-none-any.whl (606kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 54.4MB/s \n",
            "\u001b[?25hCollecting confuse>=1.0.0 (from pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/6f/90e860cba937c174d8b3775729ccc6377eb91f52ad4eeb008e7252a3646d/confuse-1.0.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: astropy in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.6/dist-packages (from missingno>=0.4.2->pandas-profiling) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: numba>=0.38.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (0.40.1)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert>=5.3.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.6.0)\n",
            "Collecting pytest>=4.0.2 (from phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/19/d5f71752f71451ccc5ed5f6739e9da4a235f38783fdaf3629cae41b2ca7b/pytest-5.1.2-py3-none-any.whl (224kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jupyter-client>=5.2.3 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling) (5.3.1)\n",
            "Collecting pytest-pylint>=0.13.0 (from phik>=0.9.8->pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/64/dc/6f35f114844fb12e38d60c4f3d2441a55baff7043ad4e013777dff55746c/pytest_pylint-0.14.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from confuse>=1.0.0->pandas-profiling) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (41.2.0)\n",
            "Requirement already satisfied, skipping upgrade: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling) (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1)\n",
            "Collecting pluggy<1.0,>=0.12 (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/06/ee/de89e0582276e3551df3110088bf20844de2b0e7df2748406876cc78e021/pluggy-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.20)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (19.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (7.2.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (4.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling) (17.0.0)\n",
            "Collecting pylint>=1.4.5 (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl (765kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=4.0.2->phik>=0.9.8->pandas-profiling) (0.6.0)\n",
            "Collecting isort<5,>=4.2.5 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 17.0MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting astroid<3,>=2.2.0 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling) (1.11.2)\n",
            "Collecting typed-ast>=1.3.0; implementation_name == \"cpython\" (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 31.4MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/26/534a6d32572a9dbca11619321535c0a7ab34688545d9d67c2c204b9e3a3d/lazy_object_proxy-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 25.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pandas-profiling, htmlmin, confuse\n",
            "  Building wheel for pandas-profiling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-profiling: filename=pandas_profiling-2.3.0-py2.py3-none-any.whl size=145035 sha256=a8c26b48b5d59ec527e8bc8568d739ba1434f5f18a639b374d1c8e579cad548e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/c7/f1/dbfef4848ebb048cb1d4a22d1ed0c62d8ff2523747235e19fe\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp36-none-any.whl size=27084 sha256=c238832ce4a76746e4e0a356dc6dfdcb319bcb7db8f0f7ae283af4fc575caa26\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
            "  Building wheel for confuse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for confuse: filename=confuse-1.0.0-cp36-none-any.whl size=17486 sha256=67fb8eff1c6d994a9ddacb7e6b58e8dfc90300e8908619838cb519649561043b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/b2/96/2074eee7dbf7b7df69d004c9b6ac4e32dad04fb7666cf943bd\n",
            "Successfully built pandas-profiling htmlmin confuse\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: category-encoders, htmlmin, pluggy, pytest, isort, mccabe, typed-ast, lazy-object-proxy, astroid, pylint, pytest-pylint, phik, confuse, pandas-profiling, plotly\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "  Found existing installation: plotly 3.6.1\n",
            "    Uninstalling plotly-3.6.1:\n",
            "      Successfully uninstalled plotly-3.6.1\n",
            "Successfully installed astroid-2.2.5 category-encoders-2.0.0 confuse-1.0.0 htmlmin-0.1.12 isort-4.3.21 lazy-object-proxy-1.4.2 mccabe-0.6.1 pandas-profiling-2.3.0 phik-0.9.8 plotly-4.1.1 pluggy-0.12.0 pylint-2.3.1 pytest-5.1.2 pytest-pylint-0.14.1 typed-ast-1.4.0\n",
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 62 (delta 2), reused 8 (delta 2), pack-reused 52\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "From https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge\n",
            " * branch            master     -> FETCH_HEAD\n",
            " * [new branch]      master     -> origin/master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJBD4ruICm1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
        "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
        "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
        "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
        "\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ7ZXxM5QHWy",
        "colab_type": "text"
      },
      "source": [
        "Do train/validate/test split with the Tanzania Waterpumps data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Amxyx3xphbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2792714-5eb5-4a78-b141-95e50ca77d98"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
        "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
        "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
        "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "train.shape, val.shape, test.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47520, 41), (11880, 41), (14358, 40))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyZBJ11ZEAjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    cols_with_zeros = ['longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "            \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sryUsD5EI6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5652f455-1215-4c9c-e2c6-8cb656ec9954"
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target & id\n",
        "train_features = train.drop(columns=[target, 'id'])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features\n",
        "print(features)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private', 'region_code', 'district_code', 'population', 'construction_year', 'basin', 'region', 'public_meeting', 'recorded_by', 'scheme_management', 'permit', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7z-PIGCEMLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIARx37-EO4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c9db108-4a9b-4860-fdff-57ac316ced2f"
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    StandardScaler(), \n",
        "    LogisticRegression(multi_class='auto', solver='lbfgs', n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Score on val\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
        "\n",
        "# Predict on test\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.7308080808080808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UAPZKCIESE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a279897-541d-475f-94ae-ca8bc711e783"
      },
      "source": [
        "# TODO\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "#     StandardScaler(), \n",
        "    DecisionTreeClassifier(random_state=42) # This line was the only thing changed from the previous code cell\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.9954755892255892\n",
            "Validation Accuracy 0.7594276094276095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xkmcjnsEkIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "65adc9ec-a190-4302-f87a-89dc1e12db0e"
      },
      "source": [
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(max_depth=16, random_state=42)\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.8486531986531987\n",
            "Validation Accuracy 0.7766835016835016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BByYtXoEHPfZ",
        "colab_type": "text"
      },
      "source": [
        "For Kaggle \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y9XHfcIHfoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = sample_submission.copy()\n",
        "submission['status_group'] = y_pred\n",
        "submission.to_csv('submission-01.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6euKWf7HgFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if in_colab:\n",
        "    from google.colab import files\n",
        "    # Just try again if you get this error:\n",
        "    # TypeError: Failed to fetch\n",
        "    # https://github.com/googlecolab/colabtools/issues/337\n",
        "    files.download('submission-01.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWV57TGQHqus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}